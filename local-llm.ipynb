{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LLM locally from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief demo about running a couple of LLMs from Hugging Face on my local machine. We can see how what kind of answers do these models provide when given the same question. Particularly we will focus on the models:\n",
    "* flan-t5-small\n",
    "* dolly-v2-3b\n",
    "* DialoGPT-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM, AutoModel, T5ForConditionalGeneration\n",
    "import accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a model-loading function, reusable for each of the considered models\n",
    "def load_model(model_id, model_type):\n",
    "    \"\"\"\n",
    "    Load a tokenizer and model from Hugging Face's Transformers library.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id (str): Identifier for the model on Hugging Face's model hub.\n",
    "    - model_type (str): Type of the model ('causal', 'seq2seq', etc.).\n",
    "\n",
    "    Returns:\n",
    "    - tokenizer: The loaded tokenizer.\n",
    "    - model: The loaded model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        if model_type == 'causal':\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        elif model_type == 'seq2seq':\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "        elif model_type == 't5':\n",
    "            model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
    "        else:\n",
    "            model = AutoModel.from_pretrained(model_id)\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the model: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flan-t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a structured prompt template to format inputs and expected responses\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "tokenizer, model = load_model('google/flan-t5-small', 't5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline creation\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 100\n",
    ")\n",
    "\n",
    "local_flan = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rome\n"
     ]
    }
   ],
   "source": [
    "# Direct Conditional Generation from the model\n",
    "print(local_flan('What is the capital of Italy?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rome\n"
     ]
    }
   ],
   "source": [
    "# Using LLMChain with the model\n",
    "llm_chain = LLMChain(prompt = prompt,\n",
    "                     llm = local_flan)\n",
    "\n",
    "question = \"What is the capital of Italy?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dolly-v2-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a structured prompt template to format inputs and expected responses\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: I believe that\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "tokenizer, model = load_model('databricks/dolly-v2-3b', 'causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline creation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model = model, \n",
    "    tokenizer = tokenizer, \n",
    "    max_length = 100\n",
    ")\n",
    "\n",
    "local_dolly = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Italy? Rome. Rome, Italy.\n"
     ]
    }
   ],
   "source": [
    "# Direct Conditional Generation from the model\n",
    "print(local_dolly('What is the capital of Italy?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Italy?\n",
      "Answer: I believe that it's the capital of Italy.\n"
     ]
    }
   ],
   "source": [
    "# Using LLMChain with the model\n",
    "llm_chain = LLMChain(prompt = prompt,\n",
    "                     llm = local_dolly)\n",
    "\n",
    "question = \"What is the capital of Italy?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DialoGPT-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a structured prompt template to format inputs and expected responses\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: I believe that\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "# model_id = \"microsoft/DialoGPT-large\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "tokenizer, model = load_model('microsoft/DialoGPT-large', 'causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline creation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model = model, \n",
    "    tokenizer = tokenizer, \n",
    "    max_length = 100\n",
    ")\n",
    "\n",
    "local_dialo = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Italy? Rome. Rome, Italy.\n"
     ]
    }
   ],
   "source": [
    "# Direct Conditional Generation from the model\n",
    "print(local_dialo('What is the capital of Italy?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital city of Italy?\n",
      "Answer: I believe that it's the city of Rome.\n"
     ]
    }
   ],
   "source": [
    "# Using LLMChain with the model\n",
    "llm_chain = LLMChain(prompt = prompt,\n",
    "                     llm = local_dialo)\n",
    "\n",
    "question = \"What is the capital city of Italy?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
